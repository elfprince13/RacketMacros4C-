@Reinclude(Headers)[= "<stdlib.h>"][= "<stdio.h>"][= "<cuda.h>"];

#include <stdlib.h>
#include <stdio.h>
#include <cuda.h>

/* Minimal declarations for CUDA support.  Testing purposes only. */

#define __constant__ __attribute__((constant))
#define __device__ __attribute__((device))
#define __global__ extern "C" __attribute__((global))
#define __host__ __attribute__((host))
#define __shared__ __attribute__((shared))
#define __launch_bounds__(...) __attribute__((launch_bounds(__VA_ARGS__)))
#define __forceinline__ __attribute__((always_inline))

typedef struct {
    unsigned int x, y, z;
} uint3;

struct dim3 {
  unsigned int x, y, z;
  __host__ __device__ dim3(unsigned x, unsigned y = 1, unsigned z = 1)
      : x(x), y(y), z(z) {}
};

uint3 __device__ extern const threadIdx;
uint3 __device__ extern const blockIdx;
dim3 __device__ extern const blockDim;
dim3 __device__ extern const gridDim;
int __device__ extern const warpSize;

// The following is some bits of the CUDA runtime, currently required for Clang
// to parse kernel invocation expressions correctly.
typedef struct cudaStream* cudaStream_t;

int cudaConfigureCall(dim3 grid_size, dim3 block_size, unsigned shared_size = 0,
                      cudaStream_t stream = 0);

@CuFunc(warpReduce)[@ WReduce][@ LogSize][@ device][volatile int *sdata;][unsigned int tid;]{
	volatile int* tdata = sdata+tid;
	@Repeat(unrollWarp)[@ I][= @Min(fromCt)[= 6][= @LogSize(unrollCt)]][= 1][= (-1)]
			(*tdata) += *(tdata + (1 << (reinterpret_cast<int>(@I(iter)) - 1)));
	
}

@CuFunc(reduce)[@ Reduce][@ LogSize][@ global][int *g_idata;][int *g_odata;][unsigned int n;]{
	int __shared__ extern *sdata;
	uint3 __device__ extern const threadIdx;
	int tid = threadIdx.x;
	*(sdata + n) = *(g_idata + n);
	@WReduce(WReduce512)[= @LogSize(oursize)][= sdata][= tid];
}

int main(int argc, char ** argv) {
	int blocksize = argc;
	switch(blocksize){
	@Repeat(unrollSwitch)[@ I][= 9][= 0][= -1]{
		case @I(caseN):
			@Reduce(reduceN)[= @I(caseN)][= (0, 1, 2)][= 0][= 0][= 0];
	}
	}
	return 1;
}
